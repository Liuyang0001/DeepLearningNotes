{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 与学习相关的技巧"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "神经网络的学习的目的是找到使损失函数的值尽可能小的参数。这是寻找最优参数的问题，解决这个问题的过程称为最优化（optimization）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 随机梯度下降法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将参数的梯度（导数）作为了线索。使用参数的梯度，沿梯度方向更新参数，并重复这个步骤多次，从而逐渐靠近最优参数，这个过程称为随机梯度下降法（stochastic gradient descent），简称SGD。用数学式可以将SGD写成如下的式子：\n",
    "\n",
    "![](https://gitee.com/liuyang0001/blogimage/raw/master/img/20200806215153.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            params[key] -= self.lr * grads[key]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参数params和grads（与之前的神经网络的实现一样）是字典型变量，按`params['W1']`、`grads['W1']`的形式，分别保存了权重参数和它们的梯度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用这个SGD类，可以按如下方式进行神经网络的参数的更新（下面的代码是不能实际运行的伪代码）。\n",
    "\n",
    "```python\n",
    "network = TwoLayerNet(...)\n",
    "optimizer = SGD()\n",
    "for i in range(10000):\n",
    "    x_batch, t_batch = get_mini_batch(...) # mini-batch\n",
    "    grads = network.gradient(x_batch, t_batch)\n",
    "    params = network.params\n",
    "    optimizer.update(params, grads)\n",
    "```\n",
    "\n",
    "这里首次出现的变量名optimizer表示“进行最优化的人”的意思，这里由SGD承担这个角色。参数的更新由optimizer负责完成。我们在这里需要做的只是将参数和梯度的信息传给optimizer。\n",
    "\n",
    "> 像这样，通过单独实现进行最优化的类，功能的模块化变得更简单。想换成其他优化的类只需要 optimizer = xxx 就可以了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD的缺点\n",
    "\n",
    "虽然SGD简单，并且容易实现，但是在解决某些问题时可能没有效率。\n",
    "\n",
    "这里，在指出SGD的缺点之际，我们来思考一下求下面这个函数的最小值的问题。\n",
    "\n",
    "![](https://gitee.com/liuyang0001/blogimage/raw/master/img/20200806220255.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如图6-1 所示，式（6.2）表示的函数是向x 轴方向延伸的“碗”状函数。实际上，式（6.2）的等高线呈向x轴方向延伸的椭圆状。\n",
    "\n",
    "![](https://gitee.com/liuyang0001/blogimage/raw/master/img/20200806220428.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果用图表示梯度的话，则如图6-2 所示。这个梯度的特征是，y 轴方向上大，x 轴方向上小。换句话说，就是y 轴方向的坡度大，而x轴方向的坡度小。这里需要注意的是，虽然函数的最小值在(x, y) = (0, 0) 处，但是图6-2 中的梯度在很多地方并没有指向(0, 0)。\n",
    "\n",
    "![](https://gitee.com/liuyang0001/blogimage/raw/master/img/20200806220633.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们来尝试对这种形状的函数应用SGD。从(x, y) = (−7.0, 2.0) 处（初始值）开始搜索，结果如图所示;\n",
    "\n",
    "![](https://gitee.com/liuyang0001/blogimage/raw/master/img/20200807163431.png)\n",
    "\n",
    "这是一个相当低效的路径。也就是说，SGD的缺点是，如果函数的形状非均向（anisotropic），比如呈延伸状，搜索的路径就会非常低效。因此，我们需要比单纯朝梯度方向前进的SGD更聪明的方法。SGD低效的根本原因是，梯度的方向并没有指向最小值的方向。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum\n",
    "\n",
    "Momentum是“动量”的意思，和物理有关。用数学式表示Momentum方法，如下：\n",
    "\n",
    "![](https://gitee.com/liuyang0001/blogimage/raw/master/img/20200809125503.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "和前面的SGD一样，W表示要更新的权重参数，$ \\frac{\\mathrm{d} L}{\\mathrm{d} W}$ 表示损失函数关于W的梯度，η 表示学习率。这里新出现了一个变量v，对应物理上的速度。\n",
    "\n",
    "式（6.3）表示了物体在梯度方向上受力，在这个力的作用下，物体的速度增加这一物理法则。如图6-4 所示，Momentum方法给人的感觉就像是小球在地面上滚动。\n",
    "\n",
    "![](https://gitee.com/liuyang0001/blogimage/raw/master/img/20200809124156.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "式（6.3）中有αv这一项。在物体不受任何力时，该项承担使物体逐渐减速的任务（α设定为0.9之类的值），对应物理上的地面摩擦或空气阻力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Momentum:\n",
    "\n",
    "    \"\"\"Momentum SGD\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():                                \n",
    "                self.v[key] = np.zeros_like(val)\n",
    "                \n",
    "        for key in params.keys():\n",
    "            self.v[key] = self.momentum*self.v[key] - self.lr*grads[key] \n",
    "            params[key] += self.v[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实例变量v会保存物体的速度。初始化时，v中什么都不保存，但当第一次调用update()时，v会以字典型变量的形式保存与参数结构相同的数据。剩余的代码部分就是将式（6.3）、式（6.4）写出来，很简单。\n",
    "现在尝试使用Momentum解决式（6.2）的最优化问题，如图6-5 所示。\n",
    "\n",
    "![](https://gitee.com/liuyang0001/blogimage/raw/master/img/20200809125834.png)\n",
    "\n",
    "图6-5 中，更新路径就像小球在碗中滚动一样。和SGD相比，我们发现“之”字形的“程度”减轻了。这是因为虽然x轴方向上受到的力非常小，但是一直在同一方向上受力，所以朝同一个方向会有一定的加速。反过来，虽然y 轴方向上受到的力很大，但是因为交互地受到正方向和反方向的力，它\n",
    "们会互相抵消，所以y 轴方向上的速度不稳定。因此，和SGD时的情形相比，可以更快地朝x轴方向靠近，减弱“之”字形的变动程度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # AdaGrad\n",
    "\n",
    "在神经网络的学习中，学习率（数学式中记为η）的值很重要。学习率过小，会导致学习花费过多时间；反过来，学习率过大，则会导致学习发散而不能正确进行。\n",
    "\n",
    "在关于学习率的有效技巧中，有一种被称为学习率衰减（learning rate decay）的方法，即随着学习的进行，使学习率逐渐减小。实际上，一开始“多”学，然后逐渐“少”学的方法，在神经网络的学习中经常被使用。逐渐减小学习率的想法，相当于将“全体”参数的学习率值一起降低。而AdaGrad进一步发展了这个想法，针对“一个一个”的参数，赋予其“定制”的值。\n",
    "\n",
    "AdaGrad会为参数的每个元素适当地调整学习率，与此同时进行学习（AdaGrad的Ada来自英文单词Adaptive，即“适当的”的意思）。下面，让我们用数学式表示AdaGrad的更新方法：\n",
    "\n",
    "![](https://gitee.com/liuyang0001/blogimage/raw/master/img/20200809130143.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "和前面的SGD一样，W表示要更新的权重参数，$ \\frac{\\mathrm{d} L}{\\mathrm{d} W}$ 表示损失函数关于W的梯度，η 表示学习率。这里新出现了变量h，如式(6.5) 所示，它保存了以前的所有梯度值的平方和（式（6.5）中的·表示对应矩阵元素的乘法）。\n",
    "\n",
    "然后，在更新参数时，通过乘以$\\frac{1}{\\sqrt{h}} $，就可以调整学习的尺度。这意味着，参数的元素中变动较大（被大幅更新）的元素的学习率将变小。也就是说，可以按参数的元素进行学习率衰减，使变动大的参数的学习率逐渐减小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> AdaGrad 会记录过去所有梯度的平方和。因此，学习越深入，更新的幅度就越小。实际上，如果无止境地学习，更新量就会变为0，完全不再更新。为了改善这个问题，可以使用RMSProp方法。RMSProp 方法并不是将过去所有的梯度一视同仁地相加，而是逐渐地遗忘过去的梯度，在做加法运算时将新梯度的信息更多地反映出来。这种操作从专业上讲，称为“指数移动平均”，呈指数函数式地减小过去的梯度的尺度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "\n",
    "    \"\"\"AdaGrad\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "            \n",
    "        for key in params.keys():\n",
    "            self.h[key] += grads[key] * grads[key]\n",
    "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 这里需要注意的是，最后一行加上了微小值1e-7。这是为了防止当self.h[key]中有0 时，将0 用作除数的情况。在很多深度学习的框架中，这个微小值也可以设定为参数，但这里我们用的是1e-7这个固定值。\n",
    "\n",
    "现在，让我们试着使用AdaGrad解决式（6.2）的最优化问题，结果如图6-6所示:\n",
    "\n",
    "![](https://gitee.com/liuyang0001/blogimage/raw/master/img/20200809132507.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由图6-6 的结果可知，函数的取值高效地向着最小值移动。由于y 轴方向上的梯度较大，因此刚开始变动较大，但是后面会根据这个较大的变动按比例进行调整，减小更新的步伐。因此，y 轴方向上的更新程度被减弱，“之”字形的变动程度有所衰减。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam\n",
    "\n",
    "Momentum参照小球在碗中滚动的物理规则进行移动，AdaGrad为参数的每个元素适当地调整更新步伐。如果将这两个方法融合在一起会怎么样呢？这就是Adam方法的基本思路A。\n",
    "\n",
    "Adam是2015 年提出的新方法。它的理论有些复杂，直观地讲，就是融合了Momentum和AdaGrad的方法。通过组合前面两个方法的优点，有望实现参数空间的高效搜索。此外，进行超参数的“偏置校正”也是Adam的特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，我们试着使用Adam解决式（6.2）的最优化问题，结果如图6-7 所示。\n",
    "\n",
    "![](https://gitee.com/liuyang0001/blogimage/raw/master/img/20200809134252.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在图6-7 中，基于Adam的更新过程就像小球在碗中滚动一样。虽然Momentun也有类似的移动，但是相比之下，Adam的小球左右摇晃的程度有所减轻。这得益于学习的更新程度被适当地调整了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Adam 会设置3 个超参数。一个是学习率（论文中以α出现），另外两个是一次momentum系数β1 和二次momentum系数β2。根据论文，标准的设定值是β1 为0.9，β2 为0.999。设置了这些值后，大多数情况下都能顺利运行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "\n",
    "    \"\"\"Adam (http://arxiv.org/abs/1412.6980v8)\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m, self.v = {}, {}\n",
    "            for key, val in params.items():\n",
    "                self.m[key] = np.zeros_like(val)\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "        \n",
    "        self.iter += 1\n",
    "        lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)         \n",
    "        \n",
    "        for key in params.keys():\n",
    "            #self.m[key] = self.beta1*self.m[key] + (1-self.beta1)*grads[key]\n",
    "            #self.v[key] = self.beta2*self.v[key] + (1-self.beta2)*(grads[key]**2)\n",
    "            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n",
    "            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])\n",
    "            \n",
    "            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)\n",
    "            \n",
    "            #unbias_m += (1 - self.beta1) * (grads[key] - self.m[key]) # correct bias\n",
    "            #unbisa_b += (1 - self.beta2) * (grads[key]*grads[key] - self.v[key]) # correct bias\n",
    "            #params[key] += self.lr * unbias_m / (np.sqrt(unbisa_b) + 1e-7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 四种方法的比较\n",
    "\n",
    "![](https://gitee.com/liuyang0001/blogimage/raw/master/img/20200809134916.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于MNIST 数据集的更新方法的比较\n",
    "我们以手写数字识别为例，比较前面介绍的SGD、Momentum、AdaGrad、Adam这4 种方法，并确认不同的方法在学习进展上有多大程度的差异。\n",
    "\n",
    "这个实验以一个5 层神经网络为对象，其中每层有100 个神经元。激活函数使用的是ReLU。先来看一下结果：\n",
    "\n",
    "![](https://gitee.com/liuyang0001/blogimage/raw/master/img/20200809135159.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从图6-9 的结果中可知，与SGD相比，其他3 种方法学习得更快，而且速度基本相同，仔细看的话，AdaGrad的学习进行得稍微快一点。这个实验需要注意的地方是，实验结果会随学习率等超参数、神经网络的结构（几层深等）的不同而发生变化。不过，一般而言，与SGD相比，其他3 种方法可以学习得更快，有时最终的识别精度也更高。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
